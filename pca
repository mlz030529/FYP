import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from concurrent.futures import ProcessPoolExecutor
import os

output_directory = ' '  # Base path for all outputs

def compute_pca(df, time_points):
    pca_results = []
    
    for time_point in time_points:
        df_subset = df[df['time(s)'] == time_point]
        if df_subset.empty:
            continue
        
        coordinates = df_subset[['x(nm)', 'y(nm)', 'z(nm)']].values
        if coordinates.shape[0] < 3:  # Need at least 3 points for PCA
            continue
        
        pca = PCA(n_components=3)
        pca.fit(coordinates)
        explained_variance = pca.explained_variance_
        
        pca_results.append({
            'time_point': time_point,
            'length': explained_variance[0],
            'width': explained_variance[1],
            'depth': explained_variance[2]
        })
        
    return pca_results

def process_file(file_path, file_index):
    print(f"Processing file: {file_path}")
    df = pd.read_csv(file_path, delimiter='\t')
    df = df[df.iloc[:, 5] == 'N']
    if df.empty:
        print(f"Warning: {file_path} is empty.")
        return []

    if df.shape[1] < 5:
        print(f"Warning: {file_path} does not have the expected number of columns.")
        return []

    # Select time points with a 1-second gap
    min_time = df['time(s)'].min()
    max_time = df['time(s)'].max()
    time_points = np.arange(min_time, max_time + 1, 1)
    
    pca_results = compute_pca(df, time_points)
    
    pca_df = pd.DataFrame(pca_results)
    pca_df.to_csv(f'{output_directory}pca_results_replicate_{file_index}.csv', index=False)
    print(f"PCA results saved to {output_directory}pca_results_replicate_{file_index}.csv")

    return pca_results

def average_pca_results(all_pca_results, time_points):
    average_results = []
    
    for time_point in time_points:
        lengths = []
        widths = []
        depths = []
        
        for pca_result in all_pca_results:
            for entry in pca_result:
                if entry['time_point'] == time_point:
                    lengths.append(entry['length'])
                    widths.append(entry['width'])
                    depths.append(entry['depth'])
                    break
        
        if not lengths:
            continue
        
        average_results.append({
            'time_point': time_point,
            'average_length': np.mean(lengths),
            'average_width': np.mean(widths),
            'average_depth': np.mean(depths)
        })
    return average_results

def process_files_parallel(files):
    all_pca_data = []
    time_points_set = set()

    with ProcessPoolExecutor() as executor:
        futures = [executor.submit(process_file, file_path, file_index) for file_index, file_path in enumerate(files, start=1)]
        for future in futures:
            result = future.result()
            if result:
                all_pca_data.append(result)
                time_points_set.update(entry['time_point'] for entry in result)

    time_points = sorted(time_points_set)
    average_results = average_pca_results(all_pca_data, time_points)
    
    average_df = pd.DataFrame(average_results)
    average_df.to_csv(f'{output_directory}average_pca_results.csv', index=False)
    print(f"Average PCA results saved to {output_directory}average_pca_results.csv")

# Paths to the files
files = ['']

# Calculate PCA for each time point in each replicate and plot
process_files_parallel(files)
